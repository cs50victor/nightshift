teams_db.spec.txt
Version: v3 (self-contained implementation spec)
Date: 2026-02-21
Status: Ready for implementation

================================================================================
SECTION A: PROJECT CONTEXT (read this first)
================================================================================

A.1) Repository layout

nightshift is a three-component project:
- daemon/   -- Rust binary (Cargo, tokio, axum). THIS is where all work happens.
- server/   -- TypeScript API server (Bun). NOT relevant to this spec.
- ui/       -- Next.js frontend. NOT relevant to this spec.

A.2) What the daemon does today

The daemon (daemon/src/main.rs) is a persistent local process that:
1. Spawns an OpenCode server (`opencode serve` on port 19276).
2. Runs an axum reverse proxy on port 19277 that forwards to OpenCode.
3. Adds daemon-specific API routes on that same proxy (teams, diffs, tools).
4. Watches ~/.claude/teams/ and ~/.claude/tasks/ JSON files for team state.
5. Self-updates from GitHub Releases.
6. Detects VM thaw (sprite hibernation) and exec-restarts.

Entry point: daemon/src/daemon.rs `run()` function.
CLI: `nightshift-daemon daemon` or `nightshift-daemon update`.

A.3) Current source files and their roles

daemon/src/main.rs       -- CLI parsing (clap), tokio::main, dispatches to daemon::run() or update
daemon/src/daemon.rs     -- Main daemon lifecycle: spawn opencode, start proxy, watchdog, teams watcher
daemon/src/config.rs     -- Loads ~/.nightshift/config.json (server_url, public_url, proxy_port)
daemon/src/proxy.rs      -- Axum router + reverse proxy to OpenCode. Has AppState with TeamsHandle.
                            Current API routes: GET /teams, GET /teams/{team}/members/{name}/diff,
                            GET /teams/{team}/members/{name}/tools, GET /project/absolute_path,
                            GET /doc (merged OpenAPI spec). Fallback proxies to OpenCode.
daemon/src/teams.rs      -- JSON-based team watcher. Watches ~/.claude/teams/ and ~/.claude/tasks/
                            via notify crate. Builds in-memory TeamState from JSON files.
                            Provides TeamsHandle = Arc<RwLock<TeamsData>>. Computes git diffs.
                            THIS FILE GETS REPLACED by DB-backed read models.
daemon/src/toolcalls.rs  -- Reads tool call history from two sources:
                            1. OpenCode's opencode.db SQLite (part table, type='tool')
                            2. Claude Code JSONL transcript files
                            Already uses rusqlite for OpenCode DB reads.
daemon/src/openapi.rs    -- Merges daemon OpenAPI spec with upstream OpenCode spec
daemon/src/nodes.rs      -- Node registration (local JSON + remote server)
daemon/src/update.rs     -- Self-update from GitHub Releases via self_update crate

daemon/tests/support/mod.rs -- Test helpers: TestHome, spawn_daemon, fake_opencode, port waiting
daemon/tests/*.rs            -- Integration tests for watchdog, exec recovery, fd hygiene, etc.

A.4) Current dependencies (Cargo.toml)


[dependencies]
clap = { version = "4", features = ["derive"] }
tokio = { version = "1", features = ["full"] }
axum = { version = "0.8", features = ["http1", "json", "tokio"] }
hyper = { version = "1", features = ["http1", "client"] }
hyper-util = { version = "0.1", features = ["tokio"] }
bytes = "1"
anyhow = "1"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
time = { version = "0.3", features = ["formatting", "parsing"] }
reqwest = { version = "0.12", default-features = false, features = ["json", "rustls-tls"] }
utoipa = { version = "5", features = ["macros"] }
utoipa-axum = "0.2"
oas3 = "0.20"
notify = "7"
sqlx = { version = "0.8", features = ["runtime-tokio", "sqlite", "migrate", "macros"] }
rusqlite = { version = "0.32", features = ["bundled"] }
semver = "1"
self_update = { version = "0.42", default-features = false, features = [...] }

[dev-dependencies]
tempfile = "3"
assert_cmd = "2"
predicates = "3"
serial_test = "3"

Key: sqlx becomes the primary SQLite integration for teams.db. rusqlite stays for read-only opencode.db access.

A.5) Current MCP setup

The daemon does NOT run MCP itself. OpenCode launches the MCP adapter as a child process
configured in daemon/opencode.json:

{
  "mcp": {
    "claude-teams": {
      "type": "local",
      "command": ["uv", "run", "--script", "__NIGHTSHIFT_CLAUDE_TEAMS_SCRIPT__"],
      "enabled": true,
      "environment": {
        "USE_TMUX_WINDOWS": "1",
        "CLAUDE_TEAMS_BACKENDS": "opencode,claude",
        "OPENCODE_SERVER_URL": "http://localhost:19277",
        "CLAUDE_TEAMS_DANGEROUSLY_SKIP_PERMISSIONS": "1"
      }
    }
  }
}

The MCP adapter (Python, FastMCP) currently:
- Manages team state via JSON files in ~/.claude/teams/ and ~/.claude/tasks/
- Spawns teammates via tmux
- Sends messages via JSON inbox files
- Has its own in-process state

After this spec: the MCP adapter becomes a thin HTTP client forwarding to daemon /internal/* endpoints.
The MCP adapter is NOT being rewritten in this spec. It stays Python/FastMCP but loses all persistence.

A.6) How daemon currently wires teams into the router

In daemon/src/daemon.rs:run():
    let teams_handle = crate::teams::new_handle();
    tokio::spawn(crate::teams::spawn_watcher(teams_handle.clone()));
    // teams_handle passed to proxy::serve()

In daemon/src/proxy.rs:
    struct AppState { ..., teams: TeamsHandle }
    // Routes call crate::teams::get_teams_summary(&state.teams).await etc.

A.7) Data dir

~/.nightshift/ is the daemon's data directory. Already created at boot in daemon.rs.
teams.db will live at ~/.nightshift/teams.db (same dir as opencode.json, nodes.json, etc.)

A.8) No JSON migration needed

Do NOT migrate existing ~/.claude/teams/ or ~/.claude/tasks/ JSON data into teams.db.
Fresh start. The JSON watcher gets removed and replaced entirely.

A.9) OpenCode's DB as a reference (not a template)

OpenCode uses SQLite (opencode.db) with Drizzle ORM (TypeScript). Tables: project, session,
message, part, todo, permission, session_share. WAL mode, FK on, busy_timeout.
The daemon already reads from opencode.db in src/toolcalls.rs via rusqlite read-only.

Our teams.db is a DIFFERENT domain. Use similar conventions (epoch ms timestamps, TEXT PKs,
WAL mode) but do not mirror OpenCode's schema.

A.10) MCP Rust SDK reference

If building an MCP server in Rust in the future, reference:
  https://github.com/modelcontextprotocol/rust-sdk/tree/main/examples/servers

Key patterns from the rmcp crate (v0.x):
- Use #[tool_router] on impl block to register tools
- Use #[tool(description = "...")] on methods
- Tool inputs via Parameters<T> where T: Deserialize + JsonSchema
- Return Result<CallToolResult, McpError>
- Implement ServerHandler trait with get_info() returning ServerCapabilities
- Use #[tool_handler] macro on ServerHandler impl
- Server started via counter.serve(transport).await

Example (calculator):
    #[derive(Debug, Clone)]
    pub struct Calculator { tool_router: ToolRouter<Self> }

    #[tool_router]
    impl Calculator {
        pub fn new() -> Self { Self { tool_router: Self::tool_router() } }

        #[tool(description = "Calculate the sum of two numbers")]
        fn sum(&self, Parameters(SumRequest { a, b }): Parameters<SumRequest>) -> String {
            (a + b).to_string()
        }
    }

    #[tool_handler]
    impl ServerHandler for Calculator {
        fn get_info(&self) -> ServerInfo {
            ServerInfo {
                instructions: Some("A simple calculator".into()),
                capabilities: ServerCapabilities::builder().enable_tools().build(),
                ..Default::default()
            }
        }
    }

This is NOT needed for Phase 1-3. Only relevant if/when replacing Python MCP with Rust MCP.


================================================================================
SECTION B: ARCHITECTURE DECISIONS
================================================================================

B.1) Authority

- teams.db (SQLite) is the ONLY source of truth for team coordination state.
- No authority in JSON files. No dual-write.

B.2) Single writer

- Daemon is the sole writer to teams.db.
- MCP adapter is stateless; forwards tool intents to daemon HTTP endpoints.
- Daemon executes, persists, and returns results.

B.3) Process ownership

- Daemon starts OpenCode server (already true).
- OpenCode's config launches MCP adapter (already true).
- MCP adapter calls daemon's /internal/* endpoints for all mutations.

B.4) Why daemon-as-authority

- Daemon is always-on and already central in the runtime.
- Avoids split-brain between MCP and daemon.
- Daemon already uses SQLite; sqlx provides async, typed row mapping, and migrations.
- Single-writer SQLite is the simplest correct architecture under these constraints.


================================================================================
SECTION C: STORAGE AND RUNTIME
================================================================================

C.1) DB path

- Default: ~/.nightshift/teams.db
- Override: NIGHTSHIFT_TEAMS_DB_PATH env var
- The ~/.nightshift/ directory is already created at daemon boot in daemon.rs

C.2) SQLite pragmas (applied on every connection open)

PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
PRAGMA foreign_keys=ON;
PRAGMA busy_timeout=5000;
PRAGMA temp_store=MEMORY;

C.3) Migration policy

- Forward-only SQL migrations.
- Tracking table: _sqlx_migrations (managed by sqlx)
- Migration files: daemon/migrations/NNNN_description.sql (e.g. 0001_teams_db.sql)
- Daemon runs migrations at boot BEFORE accepting any write requests.
- Migrations embedded and run via sqlx::migrate!().


================================================================================
SECTION D: DATA MODEL (authoritative schema)
================================================================================

All timestamps are epoch milliseconds UTC.
All IDs are TEXT (UUIDs or deterministic slugs).

D.1) Core entities

CREATE TABLE teams (
    id              TEXT PRIMARY KEY,
    name            TEXT NOT NULL UNIQUE,
    description     TEXT NOT NULL DEFAULT '',
    created_at_ms   INTEGER NOT NULL,
    archived_at_ms  INTEGER,
    lead_agent_id   TEXT NOT NULL,
    lead_session_id TEXT
);

CREATE TABLE members (
    id                  TEXT PRIMARY KEY,
    team_id             TEXT NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
    name                TEXT NOT NULL,
    agent_type          TEXT NOT NULL,
    model               TEXT NOT NULL,
    backend_type        TEXT NOT NULL,
    color               TEXT,
    cwd                 TEXT NOT NULL,
    plan_mode_required  INTEGER NOT NULL DEFAULT 0,
    tmux_target         TEXT,
    opencode_session_id TEXT,
    joined_at_ms        INTEGER NOT NULL,
    removed_at_ms       INTEGER,
    UNIQUE(team_id, name)
);

CREATE TABLE member_runs (
    id                  TEXT PRIMARY KEY,
    team_id             TEXT NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
    member_id           TEXT NOT NULL REFERENCES members(id) ON DELETE CASCADE,
    started_at_ms       INTEGER NOT NULL,
    ended_at_ms         INTEGER,
    end_reason          TEXT,       -- completed|killed|crash|timeout|unknown
    spawn_command       TEXT,
    pid                 INTEGER,
    tmux_target         TEXT,
    backend_session_ref TEXT
);
CREATE INDEX idx_member_runs_lookup ON member_runs(team_id, member_id, started_at_ms);


D.2) Task and message domain

CREATE TABLE tasks (
    id              TEXT PRIMARY KEY,
    team_id         TEXT NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
    external_task_id TEXT NOT NULL,
    subject         TEXT NOT NULL,
    description     TEXT NOT NULL,
    active_form     TEXT NOT NULL DEFAULT '',
    status          TEXT NOT NULL,  -- pending|in_progress|completed|deleted
    owner_member_id TEXT REFERENCES members(id),
    metadata_json   TEXT,
    created_at_ms   INTEGER NOT NULL,
    updated_at_ms   INTEGER NOT NULL,
    UNIQUE(team_id, external_task_id)
);

CREATE TABLE task_dependencies (
    team_id             TEXT NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
    task_id             TEXT NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    blocked_by_task_id  TEXT NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    PRIMARY KEY(team_id, task_id, blocked_by_task_id)
);

CREATE TABLE messages (
    id              TEXT PRIMARY KEY,
    team_id         TEXT NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
    from_member_id  TEXT REFERENCES members(id),
    to_member_id    TEXT REFERENCES members(id),
    message_type    TEXT NOT NULL,
    summary         TEXT,
    content_text    TEXT,
    payload_json    TEXT,
    created_at_ms   INTEGER NOT NULL
);

CREATE TABLE inbox_state (
    message_id  TEXT NOT NULL REFERENCES messages(id) ON DELETE CASCADE,
    member_id   TEXT NOT NULL REFERENCES members(id) ON DELETE CASCADE,
    read_at_ms  INTEGER,
    PRIMARY KEY(message_id, member_id)
);


D.3) Live and telemetry domain

CREATE TABLE member_status_current (
    member_id               TEXT PRIMARY KEY REFERENCES members(id) ON DELETE CASCADE,
    run_id                  TEXT REFERENCES member_runs(id),
    alive                   INTEGER NOT NULL DEFAULT 0,
    state                   TEXT NOT NULL,  -- idle|thinking|tool_running|blocked|offline
    headline                TEXT,
    active_tool_name        TEXT,
    active_tool_started_at_ms INTEGER,
    pending_from_count      INTEGER NOT NULL DEFAULT 0,
    their_unread_count      INTEGER NOT NULL DEFAULT 0,
    last_heartbeat_ms       INTEGER NOT NULL,
    last_error              TEXT
);

CREATE TABLE member_status_events (
    id              TEXT PRIMARY KEY,
    member_id       TEXT NOT NULL REFERENCES members(id) ON DELETE CASCADE,
    run_id          TEXT REFERENCES member_runs(id),
    event_type      TEXT NOT NULL,  -- state_change|heartbeat|error|output_peek
    state           TEXT,
    headline        TEXT,
    payload_json    TEXT,
    created_at_ms   INTEGER NOT NULL
);
CREATE INDEX idx_status_events_lookup ON member_status_events(member_id, created_at_ms);

CREATE TABLE tool_calls (
    id                  TEXT PRIMARY KEY,
    team_id             TEXT NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
    member_id           TEXT NOT NULL REFERENCES members(id) ON DELETE CASCADE,
    run_id              TEXT REFERENCES member_runs(id),
    backend_type        TEXT NOT NULL,
    source              TEXT NOT NULL,  -- opencode_part|claude_transcript|adapter_observer
    external_call_id    TEXT,
    tool_name           TEXT NOT NULL,
    tool_title          TEXT,
    input_summary       TEXT NOT NULL,
    input_json          TEXT,
    status              TEXT NOT NULL,  -- started|completed|error|cancelled
    error_text          TEXT,
    started_at_ms       INTEGER NOT NULL,
    ended_at_ms         INTEGER,
    duration_ms         INTEGER,
    ingested_at_ms      INTEGER NOT NULL
);
CREATE INDEX idx_tool_calls_member ON tool_calls(team_id, member_id, started_at_ms);
CREATE INDEX idx_tool_calls_team ON tool_calls(team_id, started_at_ms);
CREATE UNIQUE INDEX idx_tool_calls_dedup ON tool_calls(member_id, source, external_call_id, started_at_ms);


D.4) Audit and cursors

CREATE TABLE activity_log (
    id              TEXT PRIMARY KEY,
    team_id         TEXT NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
    member_id       TEXT REFERENCES members(id),
    kind            TEXT NOT NULL,
    payload_json    TEXT NOT NULL,
    created_at_ms   INTEGER NOT NULL
);
CREATE INDEX idx_activity_log_lookup ON activity_log(team_id, created_at_ms);

CREATE TABLE ingest_cursors (
    id              TEXT PRIMARY KEY,
    cursor_type     TEXT NOT NULL,  -- opencode_db|claude_jsonl
    member_id       TEXT,
    cursor_value    TEXT NOT NULL,
    updated_at_ms   INTEGER NOT NULL
);


================================================================================
SECTION E: WRITE-PATH INVARIANTS
================================================================================

1. One logical operation = one SQL transaction.
2. Every mutating operation appends one activity_log entry within the same transaction.
3. Daemon write API is the ONLY write entrypoint to teams.db.
4. Reads never mutate authority tables.
5. Open runs and open tool calls are reconciled on daemon restart
   (set ended_at_ms to boot time, end_reason to 'unknown', tool status to 'cancelled').


================================================================================
SECTION F: TELEMETRY INGESTION
================================================================================

F.1) Control-plane writes (synchronous)

Triggered by MCP tool operations (create team, spawn teammate, create/update task,
send message, kill teammate). Persisted immediately via daemon /internal/* endpoints.

F.2) Execution-plane ingestion (asynchronous)

Two sources, already implemented in src/toolcalls.rs (read-only today; will write to teams.db):

Source 1 - OpenCode DB (opencode.db):
  - Path: ~/Library/Application Support/opencode/opencode.db (macOS)
           $XDG_DATA_HOME/opencode/opencode.db (Linux)
  - Query: SELECT data, time_created FROM part
           WHERE session_id = ?1
           AND json_extract(data, '$.type') = 'tool'
           AND json_extract(data, '$.state.status') IN ('completed', 'error')
           ORDER BY time_created
  - Parse: json field `tool`, `state.status`, `state.input`, `state.title`,
           `state.time.start`, `state.time.end`, `state.error`
  - Already implemented in toolcalls::read_opencode_tools()

Source 2 - Claude Code JSONL transcripts:
  - Path: resolved via ~/.claude/active-sessions.json -> tmux pane matching
  - Parse: assistant messages with tool_use blocks, user messages with tool_result blocks
  - Pair by tool_use_id, compute duration from timestamps
  - Already implemented in toolcalls::read_claude_tools()

Cursor tracking:
  - Store last-seen cursor in ingest_cursors table per source+member
  - Resume from cursor on restart for idempotent ingestion

F.3) Status worker

- Poll cadence: 2s
- Heartbeat persistence: every 30s minimum
- Write member_status_current only on state change
- Append member_status_events on transition/error


================================================================================
SECTION G: API SHAPE
================================================================================

G.1) Write endpoints (called by MCP adapter, POST, JSON body/response)

POST /internal/teams/create
POST /internal/teams/delete
POST /internal/teammates/spawn
POST /internal/teammates/kill
POST /internal/tasks/create
POST /internal/tasks/update
POST /internal/messages/send
POST /internal/inbox/read          -- marks messages as read (updates read_at_ms)

G.2) Read endpoints (called by UI and MCP adapter, GET, JSON response)

GET /teams                                           -- list all teams (active + archived)
GET /teams/{team}/snapshot                           -- full team state: members, tasks, status, conflicts
GET /teams/{team}/activity?since_ms=&limit=          -- activity log entries
GET /teams/{team}/members/{name}/timeline?since_ms=&limit=  -- member status events
GET /teams/{team}/members/{name}/tools               -- tool call history for member
GET /teams/{team}/members/{name}/diff                -- git diff (kept from current implementation)

G.3) Integration with existing proxy

New routes are added to the existing axum router in proxy.rs alongside current routes.
AppState gains a db handle (Arc<sqlx::SqlitePool>).
The existing TeamsHandle (Arc<RwLock<TeamsData>>) is REMOVED once DB is live.


================================================================================
SECTION H: MCP ADAPTER CONTRACT
================================================================================

The Python MCP adapter (claude-code-teams-mcp) becomes a thin HTTP client:

- Retain existing MCP tool names (spawn_teammate, send_message, create_task, etc.)
- Adapter validates tool input shape, then POSTs to daemon /internal/* endpoint
- Adapter returns daemon JSON response directly to the model
- NO local state cache in adapter
- NO local file-based persistence (no more writing to ~/.claude/teams/, ~/.claude/tasks/)
- If daemon is unavailable, tool call fails explicitly (no silent fallback)

Environment: OPENCODE_SERVER_URL=http://localhost:19277 (the daemon proxy port)


================================================================================
SECTION I: FAILURE AND RECOVERY
================================================================================

Case A: daemon crash/restart
- DB persists on disk. On boot, daemon runs reconciliation:
  - Any member_runs with ended_at_ms IS NULL: set ended_at_ms = boot_time, end_reason = 'unknown'
  - Any tool_calls with status = 'started': set status = 'cancelled', ended_at_ms = boot_time
  - Any member_status_current: set state = 'offline', alive = 0
- Ingestion resumes from ingest_cursors

Case B: OpenCode unavailable temporarily
- Status marks source degraded (last_error in member_status_current)
- Ingestion workers retry with backoff

Case C: MCP adapter dies
- No state loss (stateless adapter)
- Restarted automatically by OpenCode session lifecycle

Case D: teammate hard kill mid-tool
- Run end reconciliation marks open tool calls as 'cancelled' with synthetic end timestamp


================================================================================
SECTION J: SECURITY AND RETENTION
================================================================================

- Default: store only metadata + input_summary (not raw input_json)
- Raw input_json: optional, gated by NIGHTSHIFT_STORE_RAW_INPUTS=1 env flag
- Redact obvious secret patterns (API keys, tokens) before persistence when raw mode enabled
- Retention jobs (run at daemon boot or periodically):
  - member_status_events: keep 30 days (configurable via NIGHTSHIFT_STATUS_RETENTION_DAYS)
  - activity_log: keep 90 days (configurable via NIGHTSHIFT_ACTIVITY_RETENTION_DAYS)
  - tool_calls: keep indefinitely by default


================================================================================
SECTION K: PERFORMANCE TARGETS
================================================================================

- Team snapshot query: p95 < 100ms for <= 50 teammates
- Timeline query: p95 < 150ms for 500 rows
- Live status freshness: <= 3s nominal


================================================================================
SECTION L: IMPLEMENTATION PLAN
================================================================================

L.1) Phase 1: DB foundation

New files:
- daemon/migrations/0001_teams_db.sql       -- all CREATE TABLE statements from Section D
- daemon/src/db.rs                          -- connection open, pragma setup, migration runner

db.rs responsibilities:
- pub async fn open(path: &Path) -> Result<sqlx::SqlitePool>
  - opens or creates SQLite file
  - applies all pragmas from Section C.2 (via execute)
  - runs pending migrations
- pub async fn run_migrations(pool: &sqlx::SqlitePool) -> Result<()>
  - uses sqlx::migrate!() to run embedded migrations
- pub async fn reconcile_on_boot(pool: &sqlx::SqlitePool) -> Result<()>
  - closes stale runs, cancels stale tool calls, marks all offline (Section I, Case A)

Changes to existing files:
- daemon/src/main.rs: add `mod db;`

L.2) Phase 2: Repository layer + daemon write/read APIs

New files:
- daemon/src/teams_repo.rs                 -- typed read/write operations against teams.db

teams_repo.rs responsibilities:
- All functions take &sqlx::SqlitePool
- Write functions: create_team, delete_team, add_member, remove_member,
  create_run, end_run, create_task, update_task, send_message, mark_read,
  upsert_status, append_status_event, insert_tool_call, update_tool_call,
  append_activity_log
- Read functions: list_teams, get_team_snapshot, get_activity, get_member_timeline,
  get_member_tools, get_member_status
- Every write function also inserts an activity_log row in the same transaction

Changes to existing files:
- daemon/src/main.rs: add `mod teams_repo;`
- daemon/src/proxy.rs:
  - Add new routes for /internal/* write endpoints and new read endpoints
  - Add DB connection/pool to AppState (replace TeamsHandle)
  - Wire routes to teams_repo functions
- daemon/src/daemon.rs:
  - Call db::open() at boot, before proxy::serve()
  - Pass DB handle to proxy::serve() instead of (or alongside) TeamsHandle
  - Remove or gate the JSON watcher spawn

L.3) Phase 3: Ingestion workers

Changes to existing files:
- daemon/src/toolcalls.rs:
  - Add ingestion functions that read from opencode.db and Claude JSONL,
    then write to teams.db tool_calls table via teams_repo
  - Add cursor read/write via ingest_cursors table
  - Current read_opencode_tools() and read_claude_tools() logic stays as parsing helpers
- daemon/src/daemon.rs:
  - Spawn ingestion worker as tokio task (runs every 2-5s)
  - Spawn status worker as tokio task (runs every 2s)

L.4) Phase 4: MCP adapter thinning

Changes to external repo (claude-code-teams-mcp):
- Remove all JSON file writes (teams config, tasks, inbox)
- Replace with HTTP calls to daemon /internal/* endpoints
- All tool handlers become: validate input -> POST to daemon -> return response

L.5) Phase 5: Cleanup

Changes to existing files:
- daemon/src/teams.rs: DELETE this file entirely (or gut to re-export types only)
  - All JSON watching, file parsing, archive logic removed
  - TeamSummary, MemberSummary, etc. response types move to teams_repo.rs or a types module
- daemon/src/proxy.rs: Remove TeamsHandle from AppState, use only DB handle
- daemon/src/daemon.rs: Remove teams::new_handle() and teams::spawn_watcher() calls


================================================================================
SECTION M: EXISTING CODE PATTERNS TO FOLLOW
================================================================================

M.1) Error handling: use anyhow::Result throughout. Existing code uses anyhow consistently.

M.2) Logging: use tracing::{info, warn, debug, error}. Already configured with env-filter.

M.3) Serialization: serde + serde_json. Response types derive Serialize, Deserialize, ToSchema.
     Use #[serde(rename_all = "camelCase")] for JSON API responses (matches existing pattern).

M.4) OpenAPI: use utoipa::ToSchema on response types and #[utoipa::path(...)] on handlers.
     Register routes via utoipa_axum::routes!() macro (see proxy.rs api_router()).

M.5) Tests: unit tests as #[cfg(test)] mod tests inside the source file. Integration tests
     in daemon/tests/. Use tempfile for temp dirs. Test names: should_<action>_when_<condition>.

M.6) sqlx usage: use query_as! / query! macros where feasible for typed mapping.
     - For dynamic queries, use query_as with manual column mapping.
     - Use transactions for multi-table writes.
     - Handle errors with tracing::warn and return empty/default where appropriate.

M.7) Time: use std::time::SystemTime for epoch ms. Existing pattern in teams.rs:
     std::time::SystemTime::now().duration_since(UNIX_EPOCH).unwrap_or_default().as_millis() as u64

M.8) IDs: generate with format!("{}-{}", prefix, &uuid[..8]) or similar short deterministic IDs.
     No external UUID crate currently; can use timestamp + random suffix or add `uuid` crate.


================================================================================
SECTION N: WHAT NOT TO DO
================================================================================

- Do NOT migrate existing JSON data from ~/.claude/teams/ or ~/.claude/tasks/
- Do NOT add JSON backward-compatibility exports
- Do NOT make MCP the authoritative writer
- Do NOT add multi-client accommodation complexity
- Do NOT rewrite the MCP adapter in Rust (keep Python/FastMCP, just thin it)
- Do NOT change the proxy port or OpenCode port
- Do NOT modify the self-update, watchdog, or node registration logic
- Do NOT add new crate dependencies unless strictly necessary (sqlx is already present for teams.db)
